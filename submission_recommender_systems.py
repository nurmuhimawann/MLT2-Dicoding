# -*- coding: utf-8 -*-
"""Submission Recommender Systems.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zEMEFlqLbc8ashoFzVpVXkIIzOtyvJTH

# **Profile** <br>
*Nama: Nur Muhammad Himawan* <br>
*Path: Machine Learning & Front End Development* <br>
*Progam: Studi Independen Batch 3 - Kampus Merdeka*

# **Proyek Akhir : Membuat Model Sistem Rekomendasi**

Selamat! Akhirnya Anda telah sampai di penghujung pembelajaran.

Untuk lulus dari kelas ini, Anda harus mengirimkan submission Proyek Akhir: Membuat Model Sistem Rekomendasi. Dalam proyek akhir ini, Anda akan mengimplementasikan apa yang telah Anda pelajari di seluruh modul untuk membuat sistem rekomendasi dan menulis laporan proyek. 

Pilihlah topik permasalahan yang ingin Anda selesaikan dengan menerapkan teknik dan algoritma pada modul Sistem Rekomendasi. Kemudian, buatlah draf laporan proyek Anda sesuai dengan kriteria dan ketentuan pada sub modul selanjutnya.

## **A. Business Understanding**

Building machine learning models for movie recommendations

## **B. Data Understanding**

### **Preparing Dataset**

menggunakan perintah wget di Colaboratory untuk mengambil dataset dari sumber eksternal.
Dataset merupakan data sekunder yang diperoleh melalui website GroupLens dengan judul [MovieLens 25M Dataset](https://grouplens.org/datasets/movielens/#:~:text=MovieLens%2025M%20Dataset).

GroupLens Research telah mengumpulkan dan menyediakan dataset film dari situs web [MovieLens](https://movielens.org).
"""

# datasets
! wget https://files.grouplens.org/datasets/movielens/ml-25m.zip

# unzip
! unzip ml-25m.zip -d /content/data/

"""Dataset sudah ter-extract dan berada pada folder terpisah di path '/content/data/ml-25m/'

### **Data Loading**

Saya ingin melihat ada file apa saja yang terdapat pada file zip. pertama, saya akan mencari directory path dari datasetnya. kemudian menggunakan basic command 'ls' untuk menampilkan list files-nya.
"""

# current directory
! pwd

# Commented out IPython magic to ensure Python compatibility.
# change directory
# %cd data/ml-25m/

# list directory contents
! ls

"""Disini saya mendapatkan 7 files (6 files berformat csv dan README). Saya tidak tahu isi dari 6 files dataset tersebut, namun terdapat README files yang biasanya akan mendeskripsikan secara singkat isi dari dataset didalamnya. Oleh karena itu, saya akan melihat isi dari file README terlebih dahulu."""

# readme file
txt_files = '/content/data/ml-25m/README.txt'
txt_content = open(txt_files, 'r').read()
print(txt_content)

"""movies.csv merupakan dataset utama yang terdapat kumpulan data film yang akan digunakan dalam sistem rekomendasi nantinya"""

# loads library
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# ignore all future warnings
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

"""mengecek jumlah data pada masing-masing dataset"""

# define dataset variable
movies = pd.read_csv('/content/data/ml-25m/movies.csv') #data film
ratings = pd.read_csv('/content/data/ml-25m/ratings.csv') #data ratings film
tags = pd.read_csv('/content/data/ml-25m/tags.csv') # data tag film
links = pd.read_csv('/content/data/ml-25m/links.csv') #data links setiap film
genome_tags = pd.read_csv('/content/data/ml-25m/genome-tags.csv') #data relevansi tag film
genome_scores = pd.read_csv('/content/data/ml-25m/genome-scores.csv') #data skor relevansi tag film

# check and count unique values in each dataframes
print('Jumlah data film: ', len(movies.movieId.unique()))
print('Jumlah data ratings atau penilaian: ', len(ratings.userId.unique()))
print('Jumlah data tags film: ', len(tags.userId.unique()))
print('Jumlah data links film: ', len(links.movieId.unique()))
print('Jumlah data genome tags: ', len(genome_tags.tagId.unique()))
print('Jumlah data genome scores: ', len(genome_scores.movieId.unique()))

"""jumlah data pada dataset film lumayan banyak sekitar 62 ribu dan ini sangat bagus untuk mengembangkan model rekomendasi. menggunakan function .info() dapat melihat information dari dataset."""

# movies information
movies.info()

# ratings information
ratings.info()

# tags information
tags.info()

# links information
links.info()

# genome_tags information
genome_tags.info()

# genome_scores information
genome_scores.info()

"""Berdasarkan informasi diatas, saya ingin membagi 6 files tersebut menjadi 3 data secara umum yaitu films, users, dan skor relevansi films. kemudian, saya akan mencoba melihat jumlah datanya."""

# skor relevansi tag
# dataframe by tagId

# Menggabungkan seluruh tagId pada kategori genome
genome_tags_all = np.concatenate((
    genome_tags.tagId.unique(),
    genome_scores.tagId.unique()
))
 
# Mengurutkan data dan menghapus data yang sama
genome_tags_all = np.sort(np.unique(genome_tags_all))
 
print('Jumlah seluruh data genome_tags: ', len(genome_tags_all))

# ID film konsisten antara ratings.csv, tags.csv, movies.csv, dan links.csv
# (yaitu, id yang sama merujuk ke film yang sama di keempat file data ini)
# dataframe by moviesId
import numpy as np
 
# Menggabungkan seluruh movieId pada kategori movies
movies_all = np.concatenate((
    movies.movieId.unique(),
    tags.movieId.unique(),
    ratings.movieId.unique(),
    links.movieId.unique()
))
 
# Mengurutkan data dan menghapus data yang sama
movies_all = np.sort(np.unique(movies_all))
 
print('Jumlah seluruh data movies berdasarkan movieId: ', len(movies_all))

# ID pengguna konsisten antara ratings.csvdan tags.csv
# (yaitu, id yang sama merujuk ke pengguna yang sama di dua file)
# dataframe by userId
import numpy as np
 
# Menggabungkan seluruh userId pada kategori user
user_all = np.concatenate((
    ratings.userId.unique(),
    tags.userId.unique()
))
 
# Mengurutkan data dan menghapus data yang sama
user_all = np.sort(np.unique(user_all))
 
print('Jumlah seluruh data user berdasarkan userId: ', len(user_all))

"""Dari informasi diatas, jumlah data pada relevancy score sangat sedikit yaitu 1K. berbanding terbalik dengan jumlah data user dan films yang masing-masing berjumlah sekitar 62K dan 162K. maka, saya akan melanjutkan dengan menggunakan data film dan users saja.

## **C. Data Preparation**

### **Data Cleaning I**

**Films**
"""

# dataset films
movies

"""memisahkan judul films dengan tahun rilis untuk menghindari terjadinya redundansi data"""

#separate tittles by year of release
movies['year_of_release'] = movies.title.str.extract('([0-9]{4})')
movies.head()

"""menghilangkan tahun rilis pada kolom judul films"""

#convert column to string
movies['title'] = movies['title'].astype(str)

#remove year
movies['title'] = movies['title'].str.split('(', 1).str[0].str.strip()
movies.head()

"""Untuk sementara, dataframe films sudah terlihat cukup baik.

**Users ratings**
"""

# dataset user ratings
ratings

"""mengecek isi data pada kolom ratings"""

# check values in rating columns
ratings.rating.unique()

"""ternyata kolom ratings memiliki sebaran data yang tidak normal, dimana data rating memiliki skala 0.5 sampai 5 dengan perbedaan 0.5 setiap skalanya. saya akan membulatkannya agar memiliki 5 nilai skala yaitu 1 - 5."""

# round values
ratings['rating'] = ratings['rating'].apply(np.ceil)

# recheck values in rating columns
ratings.rating.unique()

"""mengonversi timestamp menjadi datetime. timestamp merupakan informasi waktu yang disandikan dengan serangkaian karakter yang mengidentifikasi kapan peristiwa terjadi dengan memberikan tanggal dan waktu, yang dapat akurat hingga sepersekian detik. """

# unit='s' to convert it into epoch time
import datetime

ratings.timestamp = pd.to_datetime(ratings['timestamp'], unit='s')
ratings.head()

"""Untuk sementara, dataframe user ratings sudah terlihat cukup baik.

### **Data Merging & Cleaning II**

**Films and User ratings**

Selanjutnya adalah menggabungkan kedua dataframe sebelumnya (films and user ratings) menjadi satu dataframe yang utuh.
"""

# merge dataframe
films = pd.merge(movies, ratings, on='movieId', how='left')
films

"""memeriksa data yang hilang, null atau kosong dan membersihkannya."""

# check missing values
(films.isnull() | films.empty | films.isna()).sum()

# handling missing values
films = films.dropna()
films

# recheck missing values
(films.isnull() | films.empty | films.isna()).sum()

"""Sekilas, saat handling missing values pada proses sebelumnya, terdapat data '(no genres listed)' pada kolom genre films. """

# show genre films
import sys

np.set_printoptions(threshold=sys.maxsize)
print('Banyak genre films: ', len(films['genres'].unique()))
print('Genre films: ', films['genres'].unique())
np.set_printoptions(threshold=None)

"""ternyata memang benar terdapat **'(no genres listed)'** yang mengindikasikan ada beberapa films yang tidak memiliki genre. pendekatan **content-based filterring** akan diterapkan dalam proyek ini. maka, genre film akan menjadi fitur yang sangat penting. sebelumnya, saya akan mengecek terlebih dahulu seberapa banyak film yang tidak memiliki genre."""

# show non-genre
films[films['genres']=='(no genres listed)']

"""terdapat 23K yang jumlah data films yang tidak memiliki genre, saya akan menghapusnya karna cukup sedikit mengingat data yang dimiliki berjumlah sekitar 25 juta."""

# clean non-genre
films = films[(films.genres != '(no genres listed)')]
films.head()

"""dari 25 juta data sebelumnya, saya memiliki asumsi ada beberapa films yang mendapatkan sedikit review dari users, dengan asumsi rating terendah adalah 1 dan rating tertinggi adalah 5. maka, saya akan membuang film yang mendapatkan penilaian kurang dari 100 pereview (jumlah rating 50-250) yaitu yang setidaknya memiliki rating kurang dari 50."""

films.groupby('movieId').sum()

# remove the low rating
get_values=films['movieId'].value_counts()
temp = films.movieId.value_counts()[get_values>=50].index
films = films[films['movieId'].isin(temp)]
films

"""Data yang ada berjumlah sekitar 24.6 juta, saya memiliki asumsi terdapat data yang duplikat didalamnya."""

# duplicated by movieId
films.duplicated('movieId').sum()

# duplicated by title
films.duplicated('title').sum()

"""terdapat banyak duplikasi data didalamnya, selanjutnya saya akan menghapusnya."""

# drop duplicated data by movieId & title
films = films.drop_duplicates('movieId')
films = films.drop_duplicates('title')

"""Sekilas ketika menampilkan genre films sebelumnya, saya mendapati genre **'Sci-Fi'**. setelah saya pahami, ternyata **'Sci-Fi'** merupakan akronim dari kata **'Science Fiction'** yang menggambarkan film fiksi ilmiah. kata **'Sci-Fi'** memiliki separator dash atau tanda pisah. Hal ini, perlu dihilangkan. apabila tidak dihilangkan maka pada tahap vektorisasi TF-IDF nantinya kata **'Sci-Fi'** akan diperlakukan sebagai 2 kata yang berbeda **'Sci'** dan **'Fi'**."""

# replace the matching strings for 'sci-fi' using regex
films = films.replace(to_replace ='[nS]ci-Fi', value = 'Scifi', regex = True)
films.head()

"""membuat variabel 'preparation' untuk menampung data yang sudah bersih."""

# create preparation variable
preparation = films
preparation.sort_values('movieId').head()

"""mengonversi data series menjadi bentuk list"""

# Convert data series ‘movieId’ to list form
film_id = preparation['movieId'].tolist()

# Convert data series ‘title’ to list form
film_name = preparation['title'].tolist()

# Convert data series ‘genres’ to list form
film_genre = preparation['genres'].tolist()

print(len(film_id))
print(len(film_name))
print(len(film_genre))

# Create dataframe using dict form of ‘film_id’, ‘film_name’, and ‘film_genre’
df_film = pd.DataFrame({
    'film_id': film_id,
    'film_name': film_name,
    'genre': film_genre
})

df_film

"""Yeay, Data akhir yang berjumlah sekitar 12K telah siap untuk digunakan ke dalam proses modelling.

## **D. Model Development**

### **Content Based Filtering**

**Assign dataframe to new variable**

meng-assign dataframe ke variabel data
"""

# data sample
data = df_film
data.sample(5)

"""**TF-IDF Vectorizer**

proses term frequency-inverse document frequency (TF-IDF) untuk mencari kata yang penting dalam kolom genre. setelah melakukan perhitungan idf akan didapatkan index. kemudian, saya akan mencoba melakukan mapping untuk menampilkan data genre-nya.

"""

# library
from sklearn.feature_extraction.text import TfidfVectorizer
 
# create object TfidfVectorizer
tf = TfidfVectorizer()
 
# idf
tf.fit(data['genre']) 
 
# mapping array
tf.get_feature_names()

# fit & transform to matrix
tfidf_matrix = tf.fit_transform(data['genre']) 
 
# show matrix dimension
tfidf_matrix.shape

"""didapatkan ukuran matrix (12608, 20). dimana terdapat 12608 jumlah data dan 20 genre films. karena hasil 'tfidf' masih berbentuk vektor maka akan saya ubah ke dalam bentuk matrix."""

# change tf-idf vector to matrix form
tfidf_matrix.todense()

"""melihat hasil matriks tf-idf untuk beberapa sample film"""

# dataframe tf-idf matrix, row: film_name, columns: genre_films
pd.DataFrame(
    tfidf_matrix.todense(), 
    columns=tf.get_feature_names(),
    index=data.film_name
).sample(20, axis=1).sample(10, axis=0)

"""Yeay, dari sini sudah didapatkan representasi fitur penting dari setiap genre films.

**Cosine Similarity**

Tahap selanjutnya adalah menghitung similarity antar film yang satu dengan film lainnya berdasarkan genre
"""

# calculate cosine similarity on matrix
from sklearn.metrics.pairwise import cosine_similarity

cosine_sim = cosine_similarity(tfidf_matrix) 
cosine_sim

"""menampilkan similarity matrix setiap film dengan menampilkan nama film dalam baris dan kolom."""

# create dataframe from the results of cosine similarity
cosine_sim_df = pd.DataFrame(cosine_sim, index=data['film_name'], columns=data['film_name'])
print('Shape:', cosine_sim_df.shape)
 
# show similarity matrix
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""**Get Recommendations**

membuat function untuk merekomendasikan film dengan memberikan top 5 rekomendasi film.
"""

# function recommendations
def film_recommendations(film_name, similarity_data=cosine_sim_df, items=data[['film_name', 'genre']], k=5): 
    # get data index
    index = similarity_data.loc[:,film_name].to_numpy().argpartition(
        range(-1, -k, -1))
    
    # retrieve data from an existing index
    closest = similarity_data.columns[index[-1:-(k+2):-1]]
    
    # drop film_name you want to search
    closest = closest.drop(film_name, errors='ignore')
 
    return pd.DataFrame(closest).merge(items).head(k)

"""melihat data sample"""

# sample data
data.sample(3)

"""pada data sample terdapat film 'Harry Potter and the Prisoner of Azkaban'. saya akan mencari film rekomendasi yang tepat untuk itu."""

# check data
data[data.film_name.eq('Harry Potter and the Prisoner of Azkaban')]

"""mencari film rekomendasi untuk 'Harry Potter and the Prisoner of Azkaban'"""

# get recommendations
film_recommendations('Harry Potter and the Prisoner of Azkaban')

"""Yeay! Sistem rekomendasi berhasil memberikan 5 rekomendasi film.

### **Collaborative Filtering**

**Data Understanding**

menyiapkan beberapa library yang dibutuhkan
"""

# loads libraries
from zipfile import ZipFile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path

"""menyiapkan dataset 'preparation' sebelumnya dan meng-assignnya ke dalam variabel 'df'"""

# read dataset 
df = preparation
df

"""**Data Preparation**

melakukan encoding pada fitur ‘userId’ dan ‘film_id’ ke dalam index
"""

# change unique values of 'userId' to list
user_ids = df['userId'].unique().tolist()
print('list userID: ', user_ids)
 
# encode 'userId'
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded userID : ', user_to_user_encoded)
 
# encoding index to 'userId'
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke userID: ', user_encoded_to_user)

# change unique values of 'movieId' to list
films_ids = df['movieId'].unique().tolist()
 
# encode 'movieId'
films_to_films_encoded = {x: i for i, x in enumerate(films_ids)}
 
# encoding index to 'movieId'
films_encoded_to_films = {i: x for i, x in enumerate(films_ids)}

"""mapping userId dan movieId ke dalam dataframe"""

# Mapping 'userId' to dataframe
df['user'] = df['userId'].map(user_to_user_encoded)
 
# Mapping 'movieId' ke dataframe
df['films'] = df['movieId'].map(films_to_films_encoded)

"""mendapatkan jumlah user, films, dan nilai user ratings"""

# get number of users
num_users = len(user_to_user_encoded)
print(num_users)
 
# get number of films
num_films = len(films_encoded_to_films)
print(num_films)
 
# change dtype
df['rating'] = df['rating'].values.astype(np.float32)
 
# get min values of rating
min_rating = min(df['rating'])
 
# get max values of rating
max_rating = max(df['rating'])
 
print('Number of User: {}, Number of Films: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_films, min_rating, max_rating
))

"""**Split Data for Training and Validation**

melakukan sampling dan pembagian data menjadi data training dan validasi.
"""

# sampling
df = df.sample(frac=1, random_state=42)
df

"""membagi data untuk data train dan validasi dengan komposisi 80/20."""

# mapping users and films data into one value
x = df[['user', 'films']].values
 
# ratings
y = df['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values
 
# split data train and validation with 80/20 composition
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)
 
print(x, y)

"""Yeay, data telah siap untuk digunakan dalam proses training model.

**Proses Training**

mengembangkan model untuk menghitung skor kecocokan antara users dan films menggunakan teknik embedding
"""

# class recommendations
class RecommenderNet(tf.keras.Model):
 
  # __init__
  def __init__(self, num_users, num_films, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_films = num_films
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.films_embedding = layers.Embedding( # layer embeddings films
        num_films,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.films_bias = layers.Embedding(num_films, 1) # layer embedding films bias
 
  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # layer embedding 2
    films_vector = self.films_embedding(inputs[:, 1]) # layer embedding 3
    films_bias = self.films_bias(inputs[:, 1]) # layer embedding 4
 
    dot_user_films = tf.tensordot(user_vector, films_vector, 2) 
 
    x = dot_user_films + user_bias + films_bias
    
    return tf.nn.sigmoid(x) # activation sigmoid

"""compile model dilakukan menggunakan BinaryCrossentropy untuk menghitung loss function, Adam (Adaptive Moment Estimation) sebagai optimizer, dan root mean squared error (RMSE) sebagai metrics evaluation."""

model = RecommenderNet(num_users, num_films, 50) # model initialization
 
# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""melakukan proses training dengan menentukan nilai epochs sebesar 25"""

# training
history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 32,
    epochs = 25,
    validation_data = (x_val, y_val)
)

"""**Metric Visualization**"""

# plot metrics evaluations
plt.plot(history.history['root_mean_squared_error'], color='blue')
plt.plot(history.history['val_root_mean_squared_error'], color='red')
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

"""berdasarkan visualisasi, pada proses training model didapatkan nilai error akhir sebesar sekitar 0.17 dan error pada data validasi sebesar 0.26

**Get Recommendations**
"""

# films
films_df = df_film
films_df.head()

# data ratings
df = films
df.head()

"""mengambil sample data user secara acak dan mendefinisikan variabel 'film_not_visited' yang merupakan daftar film yang belum pernah ditonton oleh users"""

# taking user samples
user_id = df.userId.sample(1).iloc[0]
films_visited_by_user = df[df.userId == user_id]
 
# bitwise operators (~), can be found here https://docs.python.org/3/reference/expressions.html 
films_not_visited = films_df[~films_df['film_id'].isin(films_visited_by_user.movieId.values)]['film_id'] 
films_not_visited = list(
    set(films_not_visited)
    .intersection(set(films_to_films_encoded.keys()))
)
 
films_not_visited = [[films_to_films_encoded.get(x)] for x in films_not_visited]
user_encoder = user_to_user_encoded.get(user_id)
user_films_array = np.hstack(
    ([[user_encoder]] * len(films_not_visited), films_not_visited)
)

"""membuat rekomendasi film dengan model.predict() """

# get recommendations
ratings = model.predict(user_films_array).flatten()
 
top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_films_ids = [
    films_encoded_to_films.get(films_not_visited[x][0]) for x in top_ratings_indices
]
 
print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('films with high ratings from user')
print('----' * 8)
 
top_films_user = (
    films_visited_by_user.sort_values(
        by = 'rating',
        ascending=False
    )
    .head(5)
    .movieId.values
)
 
films_df_rows = films_df[films_df['film_id'].isin(top_films_user)]
for row in films_df_rows.itertuples():
    print(row.film_name, ':', row.genre)
 
print('----' * 8)
print('Top 10 films recommendation')
print('----' * 8)
 
recommended_films = films_df[films_df['film_id'].isin(recommended_films_ids)]
for row in recommended_films.itertuples():
    print(row.film_name, ':', row.genre)

"""Yeay! Model telah bisa memberikan rekomendasi films kepada user. Dimana, hasil rekomendasi untuk userId 228 adalah film dengan genre 'Drama'. Hal ini dapat dilihat pada recommendations high ratings from user dan Top 10 films recommendation untuk user. """



"""# **Conclusion**

Disini, saya telah berhasil mengembangkan model sistem rekomendasi untuk film menggunakan dua teknik yang berbeda, yaitu Content-Based Filtering dan Collaborative Filtering. Metode collaborative filtering dapat memberikan hasil rekomendasi yang cukup sesuai dengan preferensi pengguna. sedangkan metode content-based filtering juga dapat memberikan rekomendasi film yang similar dengan film yang memiliki kemiripan genre. Namun, ada yang perlu diperhatikan bahwa dataset yang digunakan memiliki data film dengan jumlah yang terbatas, bisa jadi pada dunia nyata model tidak memberikan rekomendasi sesuai yang diinginkan pengguna, dimana ada beberapa pengguna yang menyukai rekomendasi film-film terbaru. sedangkan model bisa saja merekomendasikan film-film keluaran lama.
"""



"""# **Kriteria Penilaian**

Berikut kriteria submission yang harus Anda penuhi:

- Project merupakan hasil pekerjaan sendiri. **(done)**
- Project belum pernah digunakan untuk submission kelas Machine Learning di Dicoding dan belum pernah dipublikasikan di platform manapun. **(done)**
- Dataset yang dipakai bebas, asal bisa digunakan untuk membuat sistem rekomendasi. **(done)**
- Memberikan **dokumentasi** menggunakan **text cell** pada notebook (.ipynb) untuk menjelaskan **setiap tahapan proyek**. **(done)**
- Menentukan solusi permasalahan dengan memilih pendekatan berikut:
 - Content-based Filtering **(done)**
 - Collaborative Filtering **(done)**
- Membuat draf laporan proyek machine learning yang menjelaskan alur proyek Anda mulai dari project overview, business understanding, data understanding, data preparation, modeling, hingga tahap evaluasi. Ketentuan draf laporan proyek machine learning dapat Anda lihat pada sub modul berikutnya tentang Detail Laporan. **(done)**

Saran Submission
- Menerapkan **Rubrik/Kriteria Penilaian (Tambahan)** untuk mendapatkan skala penilaian (bintang) yang lebih tinggi. **(done)**

<br>

Detail Penilaian Submission:
- **Bintang 1**: Semua ketentuan terpenuhi, tetapi terdapat indikasi plagiat dengan menggunakan proyek orang lain dan hanya mengubah kontennya saja.
- **Bintang 2**: Semua ketentuan terpenuhi, tetapi penulisan kode dan laporan berantakan.
- **Bintang 3**: Semua ketentuan terpenuhi, penulisan kode, dan laporan cukup baik.
- **Bintang 4**: Semua ketentuan terpenuhi, menerapkan minimal tiga (3) **Rubrik Penilaian (Tambahan)** pada laporan.
- **Bintang 5**: Semua ketentuan terpenuhi, menerapkan seluruh (6) **Rubrik Penilaian (Tambahan)** pada laporan.
Jika submission Anda ditolak, maka tidak ada penilaian. Kriteria penilaian bintang di atas hanya berlaku jika submission Anda lulus. **(done)**

<br>

Tips
- Anda dapat memilih beberapa topik rekomendasi (namun tidak terbatas pada daftar) berikut:

 - Rekomendasi film **(done, i build a movie recommendation system)**
 - Rekomendasi buku
 - Rekomendasi musik
 - Rekomendasi video
 - Rekomendasi produk 
 - Rekomendasi artikel
 - Rekomendasi berita
 - dsb.

<br>

Ketentuan Berkas Submission
- Mengirimkan Submission dalam bentuk .zip yang terdiri dari 3 (tiga) berkas, yaitu:  
 - File Jupyter Notebook (.ipynb). Pastikan file Jupyter Notebook sudah dijalankan, ya. **(done)**
 - File Python (.py) **(done)**
 - File laporan dalam bentuk Markdown (.md) atau Text (.txt) **(done)**
"""